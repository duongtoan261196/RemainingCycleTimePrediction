{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bea8333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import dirname\n",
    "root_path = dirname(dirname(os.getcwd()))\n",
    "print(root_path)\n",
    "import sys\n",
    "sys.path.append(root_path + '/RemainingCycleTimePrediction/2_Scripts/')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from ax.plot.contour import plot_contour\n",
    "from ax.plot.trace import optimization_trace_single_method\n",
    "from ax.service.managed_loop import optimize\n",
    "from ax.utils.notebook.plotting import render\n",
    "\n",
    "from Event_log_processing_utils import Extract_trace_and_temporal_features, Extract_prefix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data_dir = root_path + '/RemainingCycleTimePrediction/1_Data/'\n",
    "project_dir = root_path + '/RemainingCycleTimePrediction/'\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32c7935",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f3e9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'BPIC20'\n",
    "# data_name = 'Helpdesk'\n",
    "# data_name = 'EMS3141BE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff93497",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_all = pd.read_csv(data_dir+data_name+\"_processed_all.csv\")\n",
    "tab_train= pd.read_csv(data_dir+data_name+\"_processed_train.csv\")\n",
    "tab_valid= pd.read_csv(data_dir+data_name+\"_processed_valid.csv\")\n",
    "tab_test = pd.read_csv(data_dir+data_name+\"_processed_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d35e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics of the dataset\n",
    "lines, lines_t, lines_t2, lines_t3, lines_t4 = Extract_trace_and_temporal_features(tab_all)\n",
    "print(\"num_cases: {}\".format(len(tab_all['Case_ID'].unique())))\n",
    "print(\"num_activities: {}\".format(len(tab_all[\"Activity\"].unique())))\n",
    "print(\"num_events: {}\".format(len(tab_all)))\n",
    "avglen = round(np.mean([len(x) for x in lines]), 2)\n",
    "print(\"avg_case_len: {}\".format(avglen))\n",
    "maxlen = max([len(x) for x in lines]) #find maximum line size\n",
    "print(\"max_case_len: {}\".format(maxlen))\n",
    "print(\"avg_case_duration: {}\".format(round(np.mean([sublist[-1] for sublist in lines_t2])/86400, 2)))\n",
    "print(\"max_case_duration: {}\".format(round(max([sublist[-1] for sublist in lines_t2])/86400, 2)))\n",
    "print(\"min_case_duration: {}\".format(round(min([sublist[-1] for sublist in lines_t2])/86400, 2)))\n",
    "list_unique_line = []\n",
    "for line in lines:\n",
    "    if line not in list_unique_line:\n",
    "        list_unique_line.append(line)\n",
    "print(\"variants: {}\".format(len(list_unique_line)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0540c868",
   "metadata": {},
   "source": [
    "## 2. Prepare inputs and outputs for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55090026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prepare_X_Y_remaining_time(tab, list_activities, divisor, divisor2, divisor_rt, encoder, maxlen):\n",
    "    lines, lines_t, lines_t2, lines_t3, lines_t4 = Extract_trace_and_temporal_features(tab)\n",
    "    prefixes, outputs = Extract_prefix(lines, lines_t, lines_t2, lines_t3, lines_t4)\n",
    "    num_samples = len(prefixes[0])\n",
    "#     [sentences, sentences_t, sentences_t2, sentences_t3, sentences_t4], [next_ope, next_ope_t, end_ope_t]\n",
    "    print('Vectorization...')\n",
    "    num_features = len(list_activities)+5 #1 order feature + 4 temporal features\n",
    "    print('num features: {}'.format(num_features))\n",
    "    X = np.zeros((num_samples, maxlen, num_features), dtype=np.float32)\n",
    "    Y = np.zeros(num_samples, dtype=np.float32)\n",
    "    for i, sentence in enumerate(prefixes[0]):\n",
    "        leftpad = maxlen-len(sentence)\n",
    "        end_t = outputs[2][i]\n",
    "        sentence_t = prefixes[1][i]\n",
    "        sentence_t2 = prefixes[2][i]\n",
    "        sentence_t3 = prefixes[3][i]\n",
    "        sentence_t4 = prefixes[4][i]\n",
    "        one_hot_act_matrix = encoder.transform(np.array(sentence).reshape((len(sentence), 1))).toarray()\n",
    "        for t, char in enumerate(sentence):                \n",
    "            X[i, t+leftpad, :len(list_activities)] = one_hot_act_matrix[t, :]\n",
    "            X[i, t+leftpad, len(list_activities)] = t+1 # order of the activity in the sequence {1,...,maxlen}\n",
    "            X[i, t+leftpad, len(list_activities)+1] = sentence_t[t]/divisor\n",
    "            X[i, t+leftpad, len(list_activities)+2] = sentence_t2[t]/divisor2\n",
    "            X[i, t+leftpad, len(list_activities)+3] = sentence_t3[t]/86400\n",
    "            X[i, t+leftpad, len(list_activities)+4] = sentence_t4[t]/7\n",
    "        Y[i] = end_t/divisor_rt\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfac5f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_activities = list(tab_all[\"Activity\"].unique())\n",
    "num_features = len(list_activities)+5\n",
    "#creating instance of one-hot-encoder and fit on the whole dataset\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoder.fit(np.array(list_activities).reshape((len(list_activities), 1)))\n",
    "\n",
    "lines, lines_t, lines_t2, lines_t3, lines_t4 = Extract_trace_and_temporal_features(tab_all)\n",
    "maxlen = max([len(x) for x in lines]) #find maximum line size\n",
    "lines, lines_t, lines_t2, lines_t3, lines_t4 = Extract_trace_and_temporal_features(tab_train)\n",
    "divisor = np.mean([item for sublist in lines_t for item in sublist]) #average time between events\n",
    "print('divisor: {}'.format(divisor))\n",
    "divisor2 = np.mean([item for sublist in lines_t2 for item in sublist]) #average time between current and first events\n",
    "print('divisor2: {}'.format(divisor2))\n",
    "prefixes, outputs = Extract_prefix(lines, lines_t, lines_t2, lines_t3, lines_t4)\n",
    "divisor_rt = np.mean(outputs[2])\n",
    "print('divisor_rt: {}'.format(divisor_rt))\n",
    "#Train data\n",
    "X_train, Y_train = Prepare_X_Y_remaining_time(tab_train, list_activities, divisor, divisor2, divisor_rt, encoder, maxlen)\n",
    "#Valid data\n",
    "X_valid, Y_valid = Prepare_X_Y_remaining_time(tab_valid, list_activities, divisor, divisor2, divisor_rt, encoder, maxlen)\n",
    "#Test data\n",
    "X_test, Y_test = Prepare_X_Y_remaining_time(tab_test, list_activities, divisor, divisor2, divisor_rt, encoder, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75820a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventLogData(Dataset):\n",
    "    def __init__ (self, input_x, output):\n",
    "        self.X = input_x\n",
    "        self.y = output\n",
    "        self.y = self.y.to(torch.float32)\n",
    "        self.y = self.y.reshape((len(self.y),1))\n",
    "\n",
    "    #get the number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    #get a row at a particular index in the dataset\n",
    "    def __getitem__ (self,idx):\n",
    "        return [self.X[idx],self.y[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d80602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = DataLoader(EventLogData(torch.tensor(X_valid), torch.tensor(Y_valid)),\n",
    "                                batch_size=X_valid.shape[0],\n",
    "                                shuffle=False)\n",
    "test_loader = DataLoader(EventLogData(torch.tensor(X_test), torch.tensor(Y_test)),\n",
    "                                batch_size=1,\n",
    "                                shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14456b99",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter tuning with Ax package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc7cf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the LSTM class\n",
    "class LSTM_direct(nn.Module):\n",
    "    #  Determine what layers and their order in CNN object \n",
    "    def __init__(self, parameterization):\n",
    "        super(LSTM_direct, self).__init__()\n",
    "        self.hidden_dim = parameterization.get(\"neurons\", 40)\n",
    "        self.num_layers = parameterization.get(\"layers\", 1)\n",
    "        self.droppout_prob = parameterization.get(\"dropout\", 0.2)\n",
    "                \n",
    "        self.lstm = nn.LSTM(input_size=num_features, hidden_size=self.hidden_dim, \n",
    "                            num_layers=self.num_layers, batch_first=True, dropout=self.droppout_prob)        \n",
    "        self.fc = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "    \n",
    "    # Progresses data across layers    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0) \n",
    "        init_states, init_cells = self.init_hidden(batch_size)\n",
    "        init_states = init_states.to(x.device)\n",
    "        init_cells = init_cells.to(x.device)\n",
    "        lstm_output, (last_Hidden_State, last_Cell_State) = self.lstm(x, (init_states, init_cells)) \n",
    "        out = self.fc(last_Hidden_State[-1])\n",
    "        return out\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        init_states = []\n",
    "        init_cells = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(torch.zeros(batch_size, self.hidden_dim))\n",
    "            init_cells.append(torch.zeros(batch_size, self.hidden_dim))\n",
    "        return torch.stack(init_states, dim=0), torch.stack(init_cells, dim=0)      #(num_layers, B, H)\n",
    "    \n",
    "def net_train(net, train_loader, valid_loader, parameters, dtype, device, early_stop_patience):\n",
    "    net.to(dtype=dtype, device=device)\n",
    "    min_delta = 0\n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=parameters.get(\"lr\", 0.001)) # 0.001 is used if no lr is specified    \n",
    "    num_epochs = 100 # Play around with epoch number\n",
    "    \n",
    "    # Train Network\n",
    "    not_improved_count = 0\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        training_loss = 0\n",
    "        num_train = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            # move data to proper dtype and device\n",
    "            inputs = inputs.to(dtype=dtype, device=device)\n",
    "            labels = labels.to(device=device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            output = net(inputs)\n",
    "            loss = criterion(output, labels)\n",
    "            # back prop\n",
    "            loss.backward()\n",
    "            # optimize\n",
    "            optimizer.step()\n",
    "            training_loss+= loss.item()\n",
    "            num_train+=1\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            num_valid = 0\n",
    "            validation_loss = 0\n",
    "            for i,(inputs,targets) in enumerate(valid_loader):\n",
    "                inputs,targets = inputs.to(device),targets.to(device)\n",
    "                yhat_valid = net(inputs)\n",
    "                loss_valid = criterion(yhat_valid,targets)\n",
    "                validation_loss+= loss_valid.item()\n",
    "                num_valid+= 1\n",
    "        avg_training_loss = training_loss/num_train\n",
    "        avg_validation_loss = validation_loss/num_valid        \n",
    "        print(\"Epoch: {}, Training MAE : {}, Validation loss : {}\".format(epoch,avg_training_loss,avg_validation_loss))\n",
    "        if (epoch==0): \n",
    "            best_loss = avg_validation_loss\n",
    "            best_model = copy.deepcopy(net)\n",
    "        else:\n",
    "            if (best_loss - avg_validation_loss >= min_delta):\n",
    "                best_model = copy.deepcopy(net)\n",
    "                best_loss = avg_validation_loss\n",
    "                not_improved_count = 0\n",
    "            else:\n",
    "                not_improved_count += 1\n",
    "        # Early stopping\n",
    "        if not_improved_count == early_stop_patience:\n",
    "            print(\"Validation performance didn\\'t improve for {} epochs. \"\n",
    "                            \"Training stops.\".format(early_stop_patience))\n",
    "            break\n",
    "    training_time = time.time() - start_time\n",
    "    print(\"Training time:\", training_time)\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def lstm_direct_evaluate(net, data_loader, dtype, device):\n",
    "    criterion = nn.L1Loss()\n",
    "    net.eval()\n",
    "    loss = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i,(inputs,targets) in enumerate(data_loader):\n",
    "            # move data to proper dtype and device\n",
    "            inputs = inputs.to(dtype=dtype, device=device)\n",
    "            targets = targets.to(device=device)\n",
    "            outputs = net(inputs)\n",
    "            loss += criterion(outputs,targets)\n",
    "            total += 1\n",
    "    return loss.item() / total\n",
    "\n",
    "\n",
    "def train_evaluate(parameterization):\n",
    "\n",
    "    # constructing a new training data loader allows us to tune the batch size\n",
    "    train_loader = DataLoader(EventLogData(torch.tensor(X_train), torch.tensor(Y_train)),\n",
    "                                batch_size=parameterization.get(\"batchsize\", 32),\n",
    "                                shuffle=True)\n",
    "    \n",
    "    # Get neural net\n",
    "    untrained_net = LSTM_direct(parameterization)\n",
    "    # train\n",
    "    trained_net = net_train(net=untrained_net, train_loader=train_loader, valid_loader = valid_loader, \n",
    "                            parameters=parameterization, dtype=dtype, device=device, early_stop_patience = 10)\n",
    "    \n",
    "    # return the accuracy of the model as it was trained in this run\n",
    "    return lstm_direct_evaluate(\n",
    "        net=trained_net,\n",
    "        data_loader=valid_loader,\n",
    "        dtype=dtype,\n",
    "        device=device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf66d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "best_parameters, values, experiment, model = optimize(\n",
    "    parameters=[\n",
    "        {\"name\": \"neurons\", \"type\": \"choice\", \"values\": [40, 60, 80, 100], \"value_type\": \"int\"},\n",
    "        {\"name\": \"layers\", \"type\": \"choice\", \"values\": [2, 3, 4, 5], \"value_type\": \"int\"},\n",
    "        {\"name\": \"lr\", \"type\": \"range\", \"bounds\": [1e-4, 0.1], \"value_type\": \"float\", \"log_scale\": True},\n",
    "        {\"name\": \"batchsize\", \"type\": \"choice\", \"values\": [16, 32, 64], \"value_type\": \"int\"}, \n",
    "        {\"name\": \"dropout\", \"type\": \"range\", \"bounds\": [0, 0.5], \"value_type\": \"float\"}\n",
    "    ],\n",
    "  \n",
    "    evaluation_function=train_evaluate,\n",
    "    objective_name='MAE loss',\n",
    "    minimize = True,\n",
    "    random_seed = 123,\n",
    "    total_trials = 100\n",
    ")\n",
    "\n",
    "print(best_parameters)\n",
    "means, covariances = values\n",
    "print(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b67f182",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_objectives = np.array([[trial.objective_mean for trial in experiment.trials.values()]])\n",
    "\n",
    "best_objective_plot = optimization_trace_single_method(\n",
    "    y=np.minimum.accumulate(best_objectives, axis=1),\n",
    "    title=\"Model performance vs. # of iterations\",\n",
    "    ylabel=\"MAE loss\",\n",
    ")\n",
    "render(best_objective_plot)\n",
    "\n",
    "render(plot_contour(model=model, param_x='dropout', param_y='lr', metric_name='MAE loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8c238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = experiment.fetch_data()\n",
    "df = data.df\n",
    "best_arm_name = df.arm_name[df['mean'] == df['mean'].min()].values[0]\n",
    "best_arm = experiment.arms_by_name[best_arm_name]\n",
    "best_arm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2441fdbb",
   "metadata": {},
   "source": [
    "## 4. Re-Train model with tuned hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c8f5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the LSTM class\n",
    "class LSTM_direct_model(nn.Module):\n",
    "    #  Determine what layers and their order in CNN object \n",
    "    def __init__(self, hidden_dim, num_layers, droppout_prob):\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        super(LSTM_direct_model, self).__init__()                \n",
    "        self.lstm = nn.LSTM(input_size=num_features, hidden_size=hidden_dim, \n",
    "                            num_layers=num_layers, batch_first=True, dropout=droppout_prob)        \n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    \n",
    "    # Progresses data across layers    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0) \n",
    "        init_states, init_cells = self.init_hidden(batch_size)\n",
    "        init_states = init_states.to(x.device)\n",
    "        init_cells = init_cells.to(x.device)\n",
    "        lstm_output, (last_Hidden_State, last_Cell_State) = self.lstm(x, (init_states, init_cells)) \n",
    "        out = self.fc(last_Hidden_State[-1])a\n",
    "        return out\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        init_states = []\n",
    "        init_cells = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(torch.zeros(batch_size, self.hidden_dim))\n",
    "            init_cells.append(torch.zeros(batch_size, self.hidden_dim))\n",
    "        return torch.stack(init_states, dim=0), torch.stack(init_cells, dim=0)      #(num_layers, B, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f886268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = best_arm.parameters['batchsize']\n",
    "\n",
    "train_loader = DataLoader(EventLogData(torch.tensor(X_train), torch.tensor(Y_train)),\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea5af4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_folder = project_dir + '5_Output_files/Remaining_time_prediction/'+data_name+'_Tax_LSTM_direct'\n",
    "hidden_dim = best_arm.parameters['neurons']\n",
    "num_layers = best_arm.parameters['layers']\n",
    "droppout_prob = best_arm.parameters['dropout']\n",
    "lr_value = best_arm.parameters['lr']\n",
    "min_delta = 0\n",
    "num_epochs = 100\n",
    "early_stop_patience = 10\n",
    "num_runs = 5\n",
    "# Define loss and optimizer   \n",
    "for run in range(num_runs):\n",
    "    print(\"Run: {}\".format(run+1))\n",
    "    model = LSTM_direct_model(hidden_dim, num_layers, droppout_prob)\n",
    "    model.to(dtype=dtype, device=device) \n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr_value)\n",
    "    epochs_plt = []\n",
    "    mae_plt = []\n",
    "    valid_loss_plt = []\n",
    "    not_improved_count = 0\n",
    "    # Train Network   \n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        training_loss = 0\n",
    "        num_train = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            # move data to proper dtype and device\n",
    "            inputs = inputs.to(dtype=dtype, device=device)\n",
    "            labels = labels.to(device=device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            output = model(inputs)\n",
    "            loss = criterion(output, labels)\n",
    "            # back prop\n",
    "            loss.backward()\n",
    "            # optimize\n",
    "            optimizer.step()\n",
    "            training_loss+= loss.item()\n",
    "            num_train+=1\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            num_valid = 0\n",
    "            validation_loss = 0\n",
    "            for i,(inputs,targets) in enumerate(valid_loader):\n",
    "                inputs,targets = inputs.to(device),targets.to(device)\n",
    "                yhat_valid = model(inputs)\n",
    "                loss_valid = criterion(yhat_valid,targets)\n",
    "                validation_loss+= loss_valid.item()\n",
    "                num_valid+= 1\n",
    "        avg_training_loss = training_loss/num_train\n",
    "        avg_validation_loss = validation_loss/num_valid        \n",
    "        print(\"Epoch: {}, Training MAE : {}, Validation loss : {}\".format(epoch,avg_training_loss,avg_validation_loss))\n",
    "        epochs_plt.append(epoch+1)\n",
    "        mae_plt.append(avg_training_loss)\n",
    "        valid_loss_plt.append(avg_validation_loss)\n",
    "        if (epoch==0): \n",
    "            best_loss = avg_validation_loss\n",
    "            torch.save(model.state_dict(),'{}/best_model_run_{}.pt'.format(save_folder,run+1))\n",
    "            best_model = copy.deepcopy(model)\n",
    "        else:\n",
    "            if (best_loss - avg_validation_loss >= min_delta):\n",
    "                torch.save(model.state_dict(),'{}/best_model_run_{}.pt'.format(save_folder,run+1))\n",
    "                best_model = copy.deepcopy(model)\n",
    "                best_loss = avg_validation_loss\n",
    "                not_improved_count = 0\n",
    "            else:\n",
    "                not_improved_count += 1\n",
    "        # Early stopping\n",
    "        if not_improved_count == early_stop_patience:\n",
    "            print(\"Validation performance didn\\'t improve for {} epochs. \"\n",
    "                            \"Training stops.\".format(early_stop_patience))\n",
    "            break\n",
    "    training_time = time.time() - start_time\n",
    "    print(\"Training time:\", training_time)\n",
    "    filepath = '{}/Loss_'.format(save_folder)+data_name+'_run{}.txt'.format(run)    \n",
    "    with open(filepath, 'w') as file:\n",
    "        for item in zip(epochs_plt,mae_plt,valid_loss_plt):\n",
    "            file.write(\"{}\\n\".format(item))\n",
    "        file.write(\"Running time: {}\\n\".format(training_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f79583",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189898f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines, lines_t, lines_t2, lines_t3, lines_t4 = Extract_trace_and_temporal_features(tab_test)\n",
    "prefixes, outputs = Extract_prefix(lines, lines_t, lines_t2, lines_t3, lines_t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ff3c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    model.to(device)\n",
    "    err_dict = {}\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        testing_loss_all = 0\n",
    "        num_of_minibatch = 0\n",
    "        for i,(inputs,targets) in enumerate(test_loader):\n",
    "            prefix_len = len(prefixes[0][i])\n",
    "            targets = targets.to(device)\n",
    "            yhat = model(inputs.to(device))\n",
    "            loss_mape = np.abs((targets.item() - yhat.item()/targets.item()))*100\n",
    "            criterion = nn.L1Loss()\n",
    "            loss_mae = criterion(yhat,targets).item()\n",
    "            if prefix_len not in err_dict.keys():\n",
    "                err_dict[prefix_len] = [[loss_mape, loss_mae]]\n",
    "            else:\n",
    "                err_dict[prefix_len].append([loss_mape, loss_mae])\n",
    "    return err_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111f4417",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_total_dict = {}\n",
    "print(save_folder)\n",
    "for run in range(5):\n",
    "    print(\"Run: {}\".format(run+1))\n",
    "    trained_model = LSTM_direct_model(hidden_dim, num_layers, droppout_prob)\n",
    "    trained_model.load_state_dict(torch.load('{}/best_model_run_{}.pt'.format(save_folder,run+1),\n",
    "                                         map_location=torch.device(device)))\n",
    "    err_dict = evaluate_model(trained_model)\n",
    "    \n",
    "    for key in err_dict.keys():\n",
    "        err = torch.mean(torch.tensor(err_dict[key]), axis = 0)\n",
    "        if key in err_total_dict.keys():\n",
    "            err_total_dict[key].append(torch.tensor([err[0], err[1]*divisor_rt/86400]))\n",
    "        else:\n",
    "            err_total_dict[key] = [torch.tensor([err[0], err[1]*divisor_rt/86400])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c63466",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_dict = {}\n",
    "for i,(inputs,targets) in enumerate(test_loader):\n",
    "    key = len(prefixes[0][i])\n",
    "    if key in num_samples_dict.keys():\n",
    "        num_samples_dict[key] += 1\n",
    "    else:\n",
    "        num_samples_dict[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7747a280",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_prefix_len = []\n",
    "list_num_samples = []\n",
    "list_mape_err = []\n",
    "list_mape_std = []\n",
    "list_mae_err = []\n",
    "list_mae_std = []\n",
    "for key, value in err_total_dict.items():\n",
    "    list_prefix_len.append(key)\n",
    "    list_num_samples.append(num_samples_dict[key])\n",
    "    list_mape_err.append(round(torch.stack(err_total_dict[key]).mean(axis = 0)[0].item(), 3))\n",
    "    list_mape_std.append(round(torch.stack(err_total_dict[key]).std(axis=0)[0].item(), 3))\n",
    "    list_mae_err.append(round(torch.stack(err_total_dict[key]).mean(axis = 0)[1].item(), 3))\n",
    "    list_mae_std.append(round(torch.stack(err_total_dict[key]).std(axis=0)[1].item(), 3))\n",
    "tab_result = pd.DataFrame({\"Prefix length\":list_prefix_len, \"Num samples\": list_num_samples, \n",
    "                           \"MAPE(%)\":list_mape_err, \"MAPE std\": list_mape_std,\n",
    "                           \"MAE(days)\": list_mae_err, \"MAE std\": list_mae_std})\n",
    "tab_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72914c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = tab_result[tab_result[\"Num samples\"] >= 20]\n",
    "sum(tab[\"Num samples\"]*tab[\"MAE(days)\"])/sum(tab[\"Num samples\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed1b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_result.to_csv(project_dir+\"4_Outputs/Evaluation/\"+data_name+\"_Tax_LSTM_eval.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_python3)",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
