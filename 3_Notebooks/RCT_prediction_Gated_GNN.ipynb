{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a84800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import dirname\n",
    "root_path = dirname(dirname(os.getcwd()))\n",
    "print(root_path)\n",
    "import sys\n",
    "sys.path.append(root_path + '/RemainingCycleTimePrediction/2_Scripts/')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time, datetime\n",
    "import pickle as pkl\n",
    "import copy\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GatedGraphConv, global_mean_pool\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ax.plot.contour import plot_contour\n",
    "from ax.plot.trace import optimization_trace_single_method\n",
    "from ax.service.managed_loop import optimize\n",
    "from ax.utils.notebook.plotting import render\n",
    "\n",
    "from Event_log_processing_utils import Extract_trace_and_temporal_features, Extract_prefix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data_dir = root_path + '/RemainingCycleTimePrediction/1_Data/'\n",
    "project_dir = root_path + '/RemainingCycleTimePrediction/'\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3ebc6a",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4454ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_name = 'BPIC20'\n",
    "data_name = 'Helpdesk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813ce3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_all = pd.read_csv(data_dir+data_name+\"_processed_all.csv\")\n",
    "tab_train = pd.read_csv(data_dir+data_name+\"_processed_train.csv\")\n",
    "tab_valid = pd.read_csv(data_dir+data_name+\"_processed_valid.csv\")\n",
    "tab_test = pd.read_csv(data_dir+data_name+\"_processed_test.csv\")\n",
    "tab_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fdee1e",
   "metadata": {},
   "source": [
    "## 2. Prepare inputs and outputs for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5d61e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_activities = list(tab_all[\"Activity\"].unique())\n",
    "#creating instance of one-hot-encoder and fit on the whole dataset\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoder.fit(np.array(list_activities).reshape((len(list_activities), 1)))\n",
    "\n",
    "lines, lines_t, lines_t2, lines_t3, lines_t4 = Extract_trace_and_temporal_features(tab_all)\n",
    "maxlen = max([len(x) for x in lines]) #find maximum line size\n",
    "lines, lines_t, lines_t2, lines_t3, lines_t4 = Extract_trace_and_temporal_features(tab_train)\n",
    "divisor = np.mean([item for sublist in lines_t for item in sublist]) #average time between events\n",
    "print('divisor: {}'.format(divisor))\n",
    "divisor2 = np.mean([item for sublist in lines_t2 for item in sublist]) #average time between current and first events\n",
    "print('divisor2: {}'.format(divisor2))\n",
    "prefixes, outputs = Extract_prefix(lines, lines_t, lines_t2, lines_t3, lines_t4)\n",
    "divisor_rt = np.mean(outputs[2])\n",
    "print('divisor_rt: {}'.format(divisor_rt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f15618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir+\"GGNN_\"+data_name+\"_train.pkl\", \"rb\") as f:\n",
    "    X_train, Y_train =  pkl.load(f)\n",
    "with open(data_dir+\"GGNN_\"+data_name+\"_valid.pkl\", \"rb\") as f:\n",
    "    X_valid, Y_valid =  pkl.load(f)\n",
    "with open(data_dir+\"GGNN_\"+data_name+\"_test.pkl\", \"rb\") as f:\n",
    "    X_test, Y_test =  pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320a454b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventLogData(Dataset):\n",
    "    def __init__ (self, input_x, output):\n",
    "        self.X = input_x[0]\n",
    "        self.A = input_x[1]\n",
    "        self.V = input_x[2]\n",
    "        self.y = output\n",
    "        self.y = self.y.to(torch.float32)\n",
    "        self.y = self.y.reshape((len(self.y),1))\n",
    "\n",
    "    #get the number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    #get a row at a particular index in the dataset\n",
    "    def __getitem__ (self,idx):\n",
    "        return [[self.X[idx], self.A[idx], self.V[idx]],self.y[idx]]\n",
    "    \n",
    "     # get the indices for the train and test rows\n",
    "    def get_splits(self, n_valid = 0.2):\n",
    "        train_idx,valid_idx = train_test_split(list(range(len(self.X))),test_size = n_valid, shuffle = True)\n",
    "        train = Subset(self, train_idx)\n",
    "        valid = Subset(self, valid_idx)\n",
    "        return train, valid\n",
    "    \n",
    "def my_collate(batch):\n",
    "    data = [item[0] for item in batch]\n",
    "    Y = [item[1] for item in batch]\n",
    "    return [data, Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef95629",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = DataLoader(EventLogData(X_valid, Y_valid),\n",
    "                                batch_size=len(X_valid[0]),\n",
    "                                shuffle=False, collate_fn=my_collate)\n",
    "test_loader = DataLoader(EventLogData(X_test, Y_test),\n",
    "                                batch_size=1,\n",
    "                                shuffle=False, collate_fn=my_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dc75d5",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter tuning with Ax package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3739a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model class\n",
    "class GGNN_model(nn.Module):\n",
    "    def __init__(self, parameterization):\n",
    "        super(GGNN_model, self).__init__()        \n",
    "        self.ggnn_dim = parameterization.get(\"neurons\", 15)\n",
    "        self.num_layers = parameterization.get(\"layers\", 1) \n",
    "        self.droppout_prob = parameterization.get(\"dropout\", 0.2)\n",
    "        \n",
    "        self.ggnn = GatedGraphConv(self.ggnn_dim, num_layers=self.num_layers)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(p = self.droppout_prob),\n",
    "            nn.Linear(self.ggnn_dim,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = self.droppout_prob),\n",
    "            nn.Linear(256,1),\n",
    "        )\n",
    "    \n",
    "    # Progresses data across layers    \n",
    "    def forward(self, x):\n",
    "        x = [self.ggnn(X, A.to(torch.long), V) for i, (X, A, V ) in enumerate(x)]\n",
    "        x = torch.stack([global_mean_pool(single_x, batch = None) for single_x in x])\n",
    "        x = x.squeeze(1)\n",
    "        out = self.fc(x)\n",
    "        return out\n",
    "    \n",
    "def net_train(net, train_loader, valid_loader, parameters, dtype, device, early_stop_patience):\n",
    "    net.to(dtype=dtype, device=device)\n",
    "    min_delta = 0\n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=parameters.get(\"lr\", 0.001)) # 0.001 is used if no lr is specified    \n",
    "    num_epochs = 100 # Play around with epoch number\n",
    "    \n",
    "    # Train Network\n",
    "    not_improved_count = 0\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        training_loss = 0\n",
    "        num_train = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            # move data to proper dtype and device\n",
    "            inputs = [[sub_item.to(device=device) for sub_item in item] for item in inputs]\n",
    "            labels = torch.tensor(labels).to(device=device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            output = net(inputs)\n",
    "            loss = criterion(output.reshape((1,-1)),labels.reshape((1,-1)))\n",
    "            # back prop\n",
    "            loss.backward()\n",
    "            # optimize\n",
    "            optimizer.step()\n",
    "            training_loss+= loss.item()\n",
    "            num_train+=1\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            num_valid = 0\n",
    "            validation_loss = 0\n",
    "            for i,(inputs,targets) in enumerate(valid_loader):\n",
    "                inputs = [[sub_item.to(device=device) for sub_item in item] for item in inputs]\n",
    "                targets = torch.tensor(targets).to(device=device)\n",
    "                yhat_valid = net(inputs)\n",
    "                loss_valid = criterion(yhat_valid.reshape((1,-1)),targets.reshape((1,-1)))\n",
    "                validation_loss+= loss_valid.item()\n",
    "                num_valid+= 1\n",
    "        avg_training_loss = training_loss/num_train\n",
    "        avg_validation_loss = validation_loss/num_valid        \n",
    "        print(\"Epoch: {}, Training MAE : {}, Validation loss : {}\".format(epoch,avg_training_loss,avg_validation_loss))\n",
    "        if (epoch==0): \n",
    "            best_loss = avg_validation_loss\n",
    "            best_model = copy.deepcopy(net)\n",
    "        else:\n",
    "            if (best_loss - avg_validation_loss >= min_delta):\n",
    "                best_model = copy.deepcopy(net)\n",
    "                best_loss = avg_validation_loss\n",
    "                not_improved_count = 0\n",
    "            else:\n",
    "                not_improved_count += 1\n",
    "        # Early stopping\n",
    "        if not_improved_count == early_stop_patience:\n",
    "            print(\"Validation performance didn\\'t improve for {} epochs. \"\n",
    "                            \"Training stops.\".format(early_stop_patience))\n",
    "            break\n",
    "    training_time = time.time() - start_time\n",
    "    print(\"Training time:\", training_time)\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def model_evaluate(net, data_loader, dtype, device):\n",
    "    criterion = nn.L1Loss()\n",
    "    net.eval()\n",
    "    loss = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i,(inputs,targets) in enumerate(data_loader):\n",
    "            # move data to proper dtype and device\n",
    "            inputs = [[sub_item.to(dtype=dtype, device=device) for sub_item in item] for item in inputs]\n",
    "            targets = torch.tensor(targets).to(device=device)\n",
    "            outputs = net(inputs)\n",
    "            loss += criterion(outputs,targets)\n",
    "            total += 1\n",
    "    return loss.item() / total\n",
    "\n",
    "\n",
    "def train_evaluate(parameterization):\n",
    "\n",
    "    # constructing a new training data loader allows us to tune the batch size\n",
    "    train_loader = DataLoader(EventLogData(X_train,Y_train),\n",
    "                                batch_size=parameterization.get(\"batchsize\", 32),\n",
    "                                shuffle=True, collate_fn=my_collate)\n",
    "    \n",
    "    # Get neural net\n",
    "    untrained_net = GGNN_model(parameterization)\n",
    "    # train\n",
    "    trained_net = net_train(net=untrained_net, train_loader=train_loader, valid_loader = valid_loader, \n",
    "                            parameters=parameterization, dtype=dtype, device=device, early_stop_patience = 10)\n",
    "    \n",
    "    # return the accuracy of the model as it was trained in this run\n",
    "    return model_evaluate(\n",
    "        net=trained_net,\n",
    "        data_loader=valid_loader,\n",
    "        dtype=dtype,\n",
    "        device=device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c748320",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "best_parameters, values, experiment, model = optimize(\n",
    "    parameters=[\n",
    "        {\"name\": \"neurons\", \"type\": \"choice\", \"values\": [40, 60, 80, 100], \"value_type\": \"int\"},\n",
    "        {\"name\": \"layers\", \"type\": \"choice\", \"values\": [3, 4, 5], \"value_type\": \"int\"},\n",
    "        {\"name\": \"lr\", \"type\": \"range\", \"bounds\": [1e-4, 0.01], \"value_type\": \"float\", \"log_scale\": True},\n",
    "        {\"name\": \"dropout\", \"type\": \"range\", \"bounds\": [0, 0.5], \"value_type\": \"float\"},\n",
    "        {\"name\": \"batchsize\", \"type\": \"choice\", \"values\": [16, 32, 64], \"value_type\": \"int\"}\n",
    "    ],\n",
    "  \n",
    "    evaluation_function=train_evaluate,\n",
    "    objective_name='MAE loss',\n",
    "    minimize = True,\n",
    "    random_seed = 123,\n",
    "    total_trials = 100\n",
    ")\n",
    "\n",
    "print(best_parameters)\n",
    "means, covariances = values\n",
    "print(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07913d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = experiment.fetch_data()\n",
    "df = data.df\n",
    "best_arm_name = df.arm_name[df['mean'] == df['mean'].min()].values[0]\n",
    "best_arm = experiment.arms_by_name[best_arm_name]\n",
    "best_arm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70b3c1c",
   "metadata": {},
   "source": [
    "## 4. Re-Train model with tuned hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfb7af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model class\n",
    "class GGNN_model(nn.Module):\n",
    "    def __init__(self, ggnn_dim, num_layers, droppout_prob):\n",
    "        super(GGNN_model, self).__init__()        \n",
    "        self.ggnn_dim = ggnn_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.droppout_prob = droppout_prob\n",
    "        \n",
    "        self.ggnn = GatedGraphConv(self.ggnn_dim, num_layers=self.num_layers)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(p = self.droppout_prob),\n",
    "            nn.Linear(self.ggnn_dim,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = self.droppout_prob),\n",
    "            nn.Linear(256,1),\n",
    "        )\n",
    "    \n",
    "    # Progresses data across layers    \n",
    "    def forward(self, x):\n",
    "        x = [self.ggnn(X, A.to(torch.long), V) for i, (X, A, V ) in enumerate(x)]\n",
    "        x = torch.stack([global_mean_pool(single_x, batch = None) for single_x in x])\n",
    "        x = x.squeeze(1)\n",
    "        out = self.fc(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a346aecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = best_arm.parameters['batchsize']\n",
    "ggnn_dim = best_arm.parameters['neurons']\n",
    "num_layers = best_arm.parameters['layers']\n",
    "lr_value = best_arm.parameters['lr']\n",
    "droppout_prob = best_arm.parameters['dropout']\n",
    "dtype = torch.float\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_loader = DataLoader(EventLogData(X_train,Y_train), batch_size=batch_size, \n",
    "                          shuffle=True, collate_fn=my_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648e2d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = project_dir + '5_Output_files/Remaining_time_prediction/'+data_name+'_model_Gated_GNN'\n",
    "num_epochs = 100\n",
    "early_stop_patience = 20\n",
    "min_delta = 0\n",
    "num_runs = 5\n",
    "running_time = []\n",
    "for run in range(num_runs):\n",
    "    start=datetime.datetime.now()\n",
    "    print(\"Run: {}\".format(run+1))\n",
    "    model = GGNN_model(ggnn_dim, num_layers, droppout_prob)  \n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr_value)\n",
    "\n",
    "    model = model.to(device)\n",
    "    epochs_plt = []\n",
    "    mae_plt = []\n",
    "    valid_loss_plt = []\n",
    "    not_improved_count = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        training_loss = 0\n",
    "        predictions, actuals = list(),list()\n",
    "        num_train = 0\n",
    "\n",
    "        for i, (inputs,targets) in enumerate(train_loader):\n",
    "            inputs = [[sub_item.to(device=device) for sub_item in item] for item in inputs]\n",
    "            targets = torch.tensor(targets).to(device=device)\n",
    "            \n",
    "            optimizer.zero_grad() # Clearing the gradients\n",
    "            yhat = model(inputs)\n",
    "            loss = criterion(yhat.reshape((1,-1)),targets.reshape((1,-1)))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            training_loss+= loss.item()\n",
    "            num_train+=1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            num_valid = 0\n",
    "            validation_loss = 0\n",
    "            for i,(inputs,targets) in enumerate(valid_loader):            \n",
    "                inputs = [[sub_item.to(device=device) for sub_item in item] for item in inputs]\n",
    "                targets = torch.tensor(targets).to(device=device)\n",
    "                yhat_valid = model(inputs)           \n",
    "                loss_valid = criterion(yhat_valid.reshape((1,-1)),targets.reshape((1,-1)))\n",
    "                validation_loss+= loss_valid.item()\n",
    "                num_valid+= 1\n",
    "\n",
    "        avg_training_loss = training_loss/num_train\n",
    "        avg_validation_loss = validation_loss/num_valid\n",
    "        print(\"Epoch: {}, Training MAE : {}, Validation loss : {}\".format(epoch,avg_training_loss,avg_validation_loss))\n",
    "        epochs_plt.append(epoch+1)\n",
    "        mae_plt.append(avg_training_loss)\n",
    "        valid_loss_plt.append(avg_validation_loss)\n",
    "        if (epoch==0): \n",
    "            best_loss = avg_validation_loss\n",
    "            torch.save(model.state_dict(),'{}/best_model_run_{}.pt'.format(save_folder,run+1))\n",
    "        else:\n",
    "            if (best_loss - avg_validation_loss >= min_delta):\n",
    "                torch.save(model.state_dict(),'{}/best_model_run_{}.pt'.format(save_folder,run+1))\n",
    "                best_loss = avg_validation_loss\n",
    "                not_improved_count = 0\n",
    "            else:\n",
    "                not_improved_count += 1\n",
    "\n",
    "       # Early stopping\n",
    "        if not_improved_count == early_stop_patience:\n",
    "            print(\"Validation performance didn\\'t improve for {} epochs. \"\n",
    "                            \"Training stops.\".format(early_stop_patience))\n",
    "            break\n",
    "            \n",
    "    filepath = '{}/Loss_'.format(save_folder)+data_name+'_run_{}.txt'.format(run+1)\n",
    "    with open(filepath, 'w') as file:\n",
    "        for item in zip(epochs_plt,mae_plt,valid_loss_plt):\n",
    "            file.write(\"{}\\n\".format(item))\n",
    "    running_time.append((datetime.datetime.now()-start).total_seconds())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240cf99b",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2fe8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    err_dict = {}\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        testing_loss_all = 0\n",
    "        num_of_minibatch = 0\n",
    "        for i,(inputs,targets) in enumerate(test_loader):\n",
    "            prefix_len = inputs[0][0].size(0)\n",
    "            inputs = [[sub_item.to(dtype=dtype, device=device) for sub_item in item] for item in inputs]\n",
    "            targets = torch.tensor(targets).to(device=device)\n",
    "            yhat = model(inputs)\n",
    "            loss_mape = torch.abs((targets - yhat)/targets)*100\n",
    "            criterion = nn.L1Loss()\n",
    "            loss_mae = criterion(yhat,targets).item()\n",
    "            if prefix_len not in err_dict.keys():\n",
    "                err_dict[prefix_len] = [[loss_mape, loss_mae]]\n",
    "            else:\n",
    "                err_dict[prefix_len].append([loss_mape, loss_mae])\n",
    "    return err_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede4711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_total_dict = {}\n",
    "print(save_folder)\n",
    "for run in range(5):\n",
    "    print(\"Run: {}\".format(run+1))\n",
    "    trained_model = GGNN_model(ggnn_dim, num_layers, droppout_prob)\n",
    "    trained_model = trained_model.to(device)\n",
    "    trained_model.load_state_dict(torch.load('{}/best_model_run_{}.pt'.format(save_folder,run+1),\n",
    "                                         map_location=torch.device(device)))\n",
    "    err_dict = evaluate_model(trained_model)\n",
    "    \n",
    "    for key in err_dict.keys():\n",
    "        err = torch.mean(torch.tensor(err_dict[key]), axis = 0)\n",
    "        if key in err_total_dict.keys():\n",
    "            err_total_dict[key].append(torch.tensor([err[0], err[1]*divisor_rt/86400]))\n",
    "        else:\n",
    "            err_total_dict[key] = [torch.tensor([err[0], err[1]*divisor_rt/86400])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf4211e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_dict = {}\n",
    "for i,(inputs,targets) in enumerate(test_loader):\n",
    "    key = inputs[0][0].size(0)\n",
    "    if key in num_samples_dict.keys():\n",
    "        num_samples_dict[key] += 1\n",
    "    else:\n",
    "        num_samples_dict[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa4a60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_prefix_len = []\n",
    "list_num_samples = []\n",
    "list_mape_err = []\n",
    "list_mape_std = []\n",
    "list_mae_err = []\n",
    "list_mae_std = []\n",
    "for key, value in err_total_dict.items():\n",
    "    list_prefix_len.append(key)\n",
    "    list_num_samples.append(num_samples_dict[key])\n",
    "    list_mape_err.append(round(torch.stack(err_total_dict[key]).mean(axis = 0)[0].item(), 3))\n",
    "    list_mape_std.append(round(torch.stack(err_total_dict[key]).std(axis=0)[0].item(), 3))\n",
    "    list_mae_err.append(round(torch.stack(err_total_dict[key]).mean(axis = 0)[1].item(), 3))\n",
    "    list_mae_std.append(round(torch.stack(err_total_dict[key]).std(axis=0)[1].item(), 3))\n",
    "tab_result = pd.DataFrame({\"Prefix length\":list_prefix_len, \"Num samples\": list_num_samples, \n",
    "                           \"MAPE(%)\":list_mape_err, \"MAPE std\": list_mape_std,\n",
    "                           \"MAE(days)\": list_mae_err, \"MAE std\": list_mae_std})\n",
    "tab_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927eafef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = tab_result[tab_result[\"Num samples\"] >= 20]\n",
    "sum(tab[\"Num samples\"]*tab[\"MAE(days)\"])/sum(tab[\"Num samples\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac1969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_result.to_csv(project_dir+\"4_Outputs/Evaluation/\"+data_name+\"_GGNN_eval.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_python3)",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
